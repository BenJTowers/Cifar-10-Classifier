{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1YlskL1sEvcA99OQlxlKPZ67NgangBp-Z",
      "authorship_tag": "ABX9TyMu7urG8bg9cn7t8wtKz8YS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenJTowers/Cifar-10-Classifier/blob/main/CifarClassifierPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "AE4CglYbz1LJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomCrop(32, padding=4),\n",
        "    # transforms.RandomRotation(10),\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "itnun6iv52ky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transform  # use augmentations\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform   # no augmentations\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd6oxy0R7TwU",
        "outputId": "52af1d9b-ecdd-45ff-95b0-ead68846a261"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "AaTmccXX7j2y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(\"Image batch shape:\", images.shape)\n",
        "print(\"Label batch shape:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gMozRtD7seZ",
        "outputId": "4075fdbc-55fc-4308-a4a4-fdf0a4299b3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([64, 3, 32, 32])\n",
            "Label batch shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Get another batch of training images\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Unnormalize images\n",
        "def unnormalize(img, mean, std):\n",
        "    for c in range(3):  # for each channel\n",
        "        img[c, :, :] = img[c, :, :] * std[c] + mean[c]\n",
        "    return img\n",
        "\n",
        "# Display images in a grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))  # Create 2x5 grid\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = unnormalize(images[i],mean,std)  # Unnormalize first 10 images\n",
        "    img = np.transpose(img, (1, 2, 0))  # Convert from [C, H, W] to [H, W, C]\n",
        "\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(classes[labels[i].item()])  # Show class name\n",
        "    ax.axis('off')  # Hide axes for better visualization\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "u1cM4Bfb9BPf",
        "outputId": "03326268-41c8-407b-ec69-668e5244f3f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model with four convolutional layers seperated into two blocks\n",
        "#Uses batch normalization between each convolution to keep the inputs more stable\n",
        "#Max pool after each block to reduce size and complexity\n",
        "#Dropout introduced do reduce overfitting\n",
        "#use relu for simplicity and to avoud saturation\n",
        "\n",
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn4   = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        # After two pools, 32x32 -> 8x8 => 64 channels * 8 * 8 = 4096\n",
        "        self.fc1     = nn.Linear(4096, 256)\n",
        "        self.bn_fc1  = nn.BatchNorm1d(256)\n",
        "        self.fc2     = nn.Linear(256, 10)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout_conv = nn.Dropout2d(p=0.25)  # in conv layers\n",
        "        self.dropout_fc   = nn.Dropout(p=0.5)     # in fully connected layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = nn.functional.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = nn.functional.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        # Flatten for fully connected\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mlP3oHJVMIdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = CIFAR10Net().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "EPOCHS = 25  # Start with 10; you can go higher for better accuracy\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()  # set to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()            # clear gradients\n",
        "        outputs = model(inputs)          # forward pass\n",
        "        loss = criterion(outputs, labels)# compute loss\n",
        "        loss.backward()                  # backprop\n",
        "        optimizer.step()                 # update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0:  # print every 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Evaluation on the test set after each epoch\n",
        "    model.eval()  # set to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"Finished Training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr6SxMRkNg7U",
        "outputId": "0b9ee1c5-1912-4ef3-bda5-22d7b35d1d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/25], Step [100/782], Loss: 2.1802\n",
            "Epoch [1/25], Step [200/782], Loss: 1.9815\n",
            "Epoch [1/25], Step [300/782], Loss: 1.8870\n",
            "Epoch [1/25], Step [400/782], Loss: 1.8149\n",
            "Epoch [1/25], Step [500/782], Loss: 1.7749\n",
            "Epoch [1/25], Step [600/782], Loss: 1.7540\n",
            "Epoch [1/25], Step [700/782], Loss: 1.7154\n",
            "Epoch [1/25] Test Accuracy: 47.93%\n",
            "Epoch [2/25], Step [100/782], Loss: 1.6706\n",
            "Epoch [2/25], Step [200/782], Loss: 1.6534\n",
            "Epoch [2/25], Step [300/782], Loss: 1.6507\n",
            "Epoch [2/25], Step [400/782], Loss: 1.6241\n",
            "Epoch [2/25], Step [500/782], Loss: 1.6244\n",
            "Epoch [2/25], Step [600/782], Loss: 1.5839\n",
            "Epoch [2/25], Step [700/782], Loss: 1.5846\n",
            "Epoch [2/25] Test Accuracy: 52.75%\n",
            "Epoch [3/25], Step [100/782], Loss: 1.5587\n",
            "Epoch [3/25], Step [200/782], Loss: 1.5397\n",
            "Epoch [3/25], Step [300/782], Loss: 1.5290\n",
            "Epoch [3/25], Step [400/782], Loss: 1.5308\n",
            "Epoch [3/25], Step [500/782], Loss: 1.5336\n",
            "Epoch [3/25], Step [600/782], Loss: 1.5023\n",
            "Epoch [3/25], Step [700/782], Loss: 1.5083\n",
            "Epoch [3/25] Test Accuracy: 55.17%\n",
            "Epoch [4/25], Step [100/782], Loss: 1.5120\n",
            "Epoch [4/25], Step [200/782], Loss: 1.4909\n",
            "Epoch [4/25], Step [300/782], Loss: 1.4831\n",
            "Epoch [4/25], Step [400/782], Loss: 1.4697\n",
            "Epoch [4/25], Step [500/782], Loss: 1.4538\n",
            "Epoch [4/25], Step [600/782], Loss: 1.4632\n",
            "Epoch [4/25], Step [700/782], Loss: 1.4336\n",
            "Epoch [4/25] Test Accuracy: 57.63%\n",
            "Epoch [5/25], Step [100/782], Loss: 1.4152\n",
            "Epoch [5/25], Step [200/782], Loss: 1.4401\n",
            "Epoch [5/25], Step [300/782], Loss: 1.4078\n",
            "Epoch [5/25], Step [400/782], Loss: 1.4015\n",
            "Epoch [5/25], Step [500/782], Loss: 1.3964\n",
            "Epoch [5/25], Step [600/782], Loss: 1.3940\n",
            "Epoch [5/25], Step [700/782], Loss: 1.3982\n",
            "Epoch [5/25] Test Accuracy: 59.26%\n",
            "Epoch [6/25], Step [100/782], Loss: 1.3788\n",
            "Epoch [6/25], Step [200/782], Loss: 1.3809\n",
            "Epoch [6/25], Step [300/782], Loss: 1.3685\n",
            "Epoch [6/25], Step [400/782], Loss: 1.3623\n",
            "Epoch [6/25], Step [500/782], Loss: 1.3589\n",
            "Epoch [6/25], Step [600/782], Loss: 1.3353\n",
            "Epoch [6/25], Step [700/782], Loss: 1.3650\n",
            "Epoch [6/25] Test Accuracy: 58.93%\n",
            "Epoch [7/25], Step [100/782], Loss: 1.3377\n",
            "Epoch [7/25], Step [200/782], Loss: 1.3403\n",
            "Epoch [7/25], Step [300/782], Loss: 1.3337\n",
            "Epoch [7/25], Step [400/782], Loss: 1.3313\n",
            "Epoch [7/25], Step [500/782], Loss: 1.3190\n",
            "Epoch [7/25], Step [600/782], Loss: 1.3120\n",
            "Epoch [7/25], Step [700/782], Loss: 1.3039\n",
            "Epoch [7/25] Test Accuracy: 62.71%\n",
            "Epoch [8/25], Step [100/782], Loss: 1.2814\n",
            "Epoch [8/25], Step [200/782], Loss: 1.2829\n",
            "Epoch [8/25], Step [300/782], Loss: 1.2957\n",
            "Epoch [8/25], Step [400/782], Loss: 1.2694\n",
            "Epoch [8/25], Step [500/782], Loss: 1.2792\n",
            "Epoch [8/25], Step [600/782], Loss: 1.2963\n",
            "Epoch [8/25], Step [700/782], Loss: 1.2727\n",
            "Epoch [8/25] Test Accuracy: 64.88%\n",
            "Epoch [9/25], Step [100/782], Loss: 1.2701\n",
            "Epoch [9/25], Step [200/782], Loss: 1.2547\n",
            "Epoch [9/25], Step [300/782], Loss: 1.2306\n",
            "Epoch [9/25], Step [400/782], Loss: 1.2336\n",
            "Epoch [9/25], Step [500/782], Loss: 1.2432\n",
            "Epoch [9/25], Step [600/782], Loss: 1.2358\n",
            "Epoch [9/25], Step [700/782], Loss: 1.2558\n",
            "Epoch [9/25] Test Accuracy: 65.30%\n",
            "Epoch [10/25], Step [100/782], Loss: 1.2250\n",
            "Epoch [10/25], Step [200/782], Loss: 1.2208\n",
            "Epoch [10/25], Step [300/782], Loss: 1.2367\n",
            "Epoch [10/25], Step [400/782], Loss: 1.2351\n",
            "Epoch [10/25], Step [500/782], Loss: 1.2268\n",
            "Epoch [10/25], Step [600/782], Loss: 1.2087\n",
            "Epoch [10/25], Step [700/782], Loss: 1.1907\n",
            "Epoch [10/25] Test Accuracy: 66.73%\n",
            "Epoch [11/25], Step [100/782], Loss: 1.2106\n",
            "Epoch [11/25], Step [200/782], Loss: 1.2266\n",
            "Epoch [11/25], Step [300/782], Loss: 1.1747\n",
            "Epoch [11/25], Step [400/782], Loss: 1.1798\n",
            "Epoch [11/25], Step [500/782], Loss: 1.1791\n",
            "Epoch [11/25], Step [600/782], Loss: 1.1794\n",
            "Epoch [11/25], Step [700/782], Loss: 1.1730\n",
            "Epoch [11/25] Test Accuracy: 66.19%\n",
            "Epoch [12/25], Step [100/782], Loss: 1.1831\n",
            "Epoch [12/25], Step [200/782], Loss: 1.1985\n",
            "Epoch [12/25], Step [300/782], Loss: 1.1615\n",
            "Epoch [12/25], Step [400/782], Loss: 1.1548\n",
            "Epoch [12/25], Step [500/782], Loss: 1.1476\n",
            "Epoch [12/25], Step [600/782], Loss: 1.1748\n",
            "Epoch [12/25], Step [700/782], Loss: 1.1556\n",
            "Epoch [12/25] Test Accuracy: 68.62%\n",
            "Epoch [13/25], Step [100/782], Loss: 1.1531\n",
            "Epoch [13/25], Step [200/782], Loss: 1.1582\n",
            "Epoch [13/25], Step [300/782], Loss: 1.1412\n",
            "Epoch [13/25], Step [400/782], Loss: 1.1292\n",
            "Epoch [13/25], Step [500/782], Loss: 1.1436\n",
            "Epoch [13/25], Step [600/782], Loss: 1.1526\n",
            "Epoch [13/25], Step [700/782], Loss: 1.1369\n",
            "Epoch [13/25] Test Accuracy: 69.40%\n",
            "Epoch [14/25], Step [100/782], Loss: 1.1233\n",
            "Epoch [14/25], Step [200/782], Loss: 1.1602\n",
            "Epoch [14/25], Step [300/782], Loss: 1.1303\n",
            "Epoch [14/25], Step [400/782], Loss: 1.1207\n",
            "Epoch [14/25], Step [500/782], Loss: 1.1001\n",
            "Epoch [14/25], Step [600/782], Loss: 1.1405\n",
            "Epoch [14/25], Step [700/782], Loss: 1.1015\n",
            "Epoch [14/25] Test Accuracy: 69.66%\n",
            "Epoch [15/25], Step [100/782], Loss: 1.1173\n",
            "Epoch [15/25], Step [200/782], Loss: 1.1188\n",
            "Epoch [15/25], Step [300/782], Loss: 1.0969\n",
            "Epoch [15/25], Step [400/782], Loss: 1.1153\n",
            "Epoch [15/25], Step [500/782], Loss: 1.1108\n",
            "Epoch [15/25], Step [600/782], Loss: 1.1036\n",
            "Epoch [15/25], Step [700/782], Loss: 1.1372\n",
            "Epoch [15/25] Test Accuracy: 69.37%\n",
            "Epoch [16/25], Step [100/782], Loss: 1.1002\n",
            "Epoch [16/25], Step [200/782], Loss: 1.1043\n",
            "Epoch [16/25], Step [300/782], Loss: 1.1010\n",
            "Epoch [16/25], Step [400/782], Loss: 1.0897\n",
            "Epoch [16/25], Step [500/782], Loss: 1.1104\n",
            "Epoch [16/25], Step [600/782], Loss: 1.0999\n",
            "Epoch [16/25], Step [700/782], Loss: 1.1049\n",
            "Epoch [16/25] Test Accuracy: 69.72%\n",
            "Epoch [17/25], Step [100/782], Loss: 1.0900\n",
            "Epoch [17/25], Step [200/782], Loss: 1.0852\n",
            "Epoch [17/25], Step [300/782], Loss: 1.0699\n",
            "Epoch [17/25], Step [400/782], Loss: 1.0752\n",
            "Epoch [17/25], Step [500/782], Loss: 1.0792\n",
            "Epoch [17/25], Step [600/782], Loss: 1.0881\n",
            "Epoch [17/25], Step [700/782], Loss: 1.0642\n",
            "Epoch [17/25] Test Accuracy: 70.09%\n",
            "Epoch [18/25], Step [100/782], Loss: 1.0775\n",
            "Epoch [18/25], Step [200/782], Loss: 1.0604\n",
            "Epoch [18/25], Step [300/782], Loss: 1.0706\n",
            "Epoch [18/25], Step [400/782], Loss: 1.0796\n",
            "Epoch [18/25], Step [500/782], Loss: 1.0635\n",
            "Epoch [18/25], Step [600/782], Loss: 1.0739\n",
            "Epoch [18/25], Step [700/782], Loss: 1.0588\n",
            "Epoch [18/25] Test Accuracy: 72.33%\n",
            "Epoch [19/25], Step [100/782], Loss: 1.0467\n",
            "Epoch [19/25], Step [200/782], Loss: 1.0728\n",
            "Epoch [19/25], Step [300/782], Loss: 1.0665\n",
            "Epoch [19/25], Step [400/782], Loss: 1.0414\n",
            "Epoch [19/25], Step [500/782], Loss: 1.0603\n",
            "Epoch [19/25], Step [600/782], Loss: 1.0641\n",
            "Epoch [19/25], Step [700/782], Loss: 1.0454\n",
            "Epoch [19/25] Test Accuracy: 72.11%\n",
            "Epoch [20/25], Step [100/782], Loss: 1.0390\n",
            "Epoch [20/25], Step [200/782], Loss: 1.0508\n",
            "Epoch [20/25], Step [300/782], Loss: 1.0237\n",
            "Epoch [20/25], Step [400/782], Loss: 1.0601\n",
            "Epoch [20/25], Step [500/782], Loss: 1.0203\n",
            "Epoch [20/25], Step [600/782], Loss: 1.0389\n",
            "Epoch [20/25], Step [700/782], Loss: 1.0560\n",
            "Epoch [20/25] Test Accuracy: 72.20%\n",
            "Epoch [21/25], Step [100/782], Loss: 1.0185\n",
            "Epoch [21/25], Step [200/782], Loss: 1.0356\n",
            "Epoch [21/25], Step [300/782], Loss: 1.0311\n",
            "Epoch [21/25], Step [400/782], Loss: 1.0606\n",
            "Epoch [21/25], Step [500/782], Loss: 1.0364\n",
            "Epoch [21/25], Step [600/782], Loss: 1.0023\n",
            "Epoch [21/25], Step [700/782], Loss: 1.0512\n",
            "Epoch [21/25] Test Accuracy: 71.99%\n",
            "Epoch [22/25], Step [100/782], Loss: 1.0257\n",
            "Epoch [22/25], Step [200/782], Loss: 1.0278\n",
            "Epoch [22/25], Step [300/782], Loss: 1.0135\n",
            "Epoch [22/25], Step [400/782], Loss: 1.0194\n",
            "Epoch [22/25], Step [500/782], Loss: 1.0203\n",
            "Epoch [22/25], Step [600/782], Loss: 1.0285\n",
            "Epoch [22/25], Step [700/782], Loss: 1.0075\n",
            "Epoch [22/25] Test Accuracy: 72.88%\n",
            "Epoch [23/25], Step [100/782], Loss: 1.0104\n",
            "Epoch [23/25], Step [200/782], Loss: 1.0200\n",
            "Epoch [23/25], Step [300/782], Loss: 1.0169\n",
            "Epoch [23/25], Step [400/782], Loss: 0.9732\n",
            "Epoch [23/25], Step [500/782], Loss: 0.9971\n",
            "Epoch [23/25], Step [600/782], Loss: 1.0195\n",
            "Epoch [23/25], Step [700/782], Loss: 1.0254\n",
            "Epoch [23/25] Test Accuracy: 73.15%\n",
            "Epoch [24/25], Step [100/782], Loss: 1.0095\n",
            "Epoch [24/25], Step [200/782], Loss: 1.0091\n",
            "Epoch [24/25], Step [300/782], Loss: 1.0011\n",
            "Epoch [24/25], Step [400/782], Loss: 1.0052\n",
            "Epoch [24/25], Step [500/782], Loss: 1.0250\n",
            "Epoch [24/25], Step [600/782], Loss: 1.0111\n",
            "Epoch [24/25], Step [700/782], Loss: 0.9772\n",
            "Epoch [24/25] Test Accuracy: 74.36%\n",
            "Epoch [25/25], Step [100/782], Loss: 1.0010\n",
            "Epoch [25/25], Step [200/782], Loss: 0.9793\n",
            "Epoch [25/25], Step [300/782], Loss: 1.0062\n",
            "Epoch [25/25], Step [400/782], Loss: 0.9949\n",
            "Epoch [25/25], Step [500/782], Loss: 0.9835\n",
            "Epoch [25/25], Step [600/782], Loss: 0.9911\n",
            "Epoch [25/25], Step [700/782], Loss: 0.9937\n",
            "Epoch [25/25] Test Accuracy: 74.25%\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeeperCIFAR10Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperCIFAR10Net, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Block 2\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4   = nn.BatchNorm2d(64)\n",
        "\n",
        "        # NEW: Extra block for slightly deeper network\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5   = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn6   = nn.BatchNorm2d(128)\n",
        "\n",
        "        # After 3 pools, image goes from 32x32 -> 4x4\n",
        "        # Final channels = 128, so flatten size = 128 * 4 * 4 = 2048\n",
        "        self.fc1 = nn.Linear(128*4*4, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        # Dropout with higher rates for stronger regularization\n",
        "        self.dropout_conv = nn.Dropout2d(p=0.4)  # conv layers\n",
        "        self.dropout_fc   = nn.Dropout(p=0.6)    # fully connected layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
        "        x = nn.functional.max_pool2d(x, 2)  # 32->16\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
        "        x = nn.functional.relu(self.bn4(self.conv4(x)))\n",
        "        x = nn.functional.max_pool2d(x, 2)  # 16->8\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        # Block 3 (extra block)\n",
        "        x = nn.functional.relu(self.bn5(self.conv5(x)))\n",
        "        x = nn.functional.relu(self.bn6(self.conv6(x)))\n",
        "        x = nn.functional.max_pool2d(x, 2)  # 8->4\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 128*4*4)\n",
        "\n",
        "        # FC layers\n",
        "        x = nn.functional.relu(self.bn_fc1(self.fc1(x)))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2rilSKY_U8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = DeeperCIFAR10Net().to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# StepLR reduces LR by a factor (gamma=0.1) every step_size epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print every 100 batches\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Update the scheduler each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"Finished Training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Xub9pSVB3-",
        "outputId": "6e837130-1cf8-4cfd-a047-8939809bf977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "DeeperCIFAR10Net(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "  (bn_fc1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (dropout_conv): Dropout2d(p=0.4, inplace=False)\n",
            "  (dropout_fc): Dropout(p=0.6, inplace=False)\n",
            ")\n",
            "Epoch [1/25], Step [100/782], Loss: 2.1184\n",
            "Epoch [1/25], Step [200/782], Loss: 1.9201\n",
            "Epoch [1/25], Step [300/782], Loss: 1.8290\n",
            "Epoch [1/25], Step [400/782], Loss: 1.7848\n",
            "Epoch [1/25], Step [500/782], Loss: 1.7107\n",
            "Epoch [1/25], Step [600/782], Loss: 1.6739\n",
            "Epoch [1/25], Step [700/782], Loss: 1.6167\n",
            "Epoch [1/25] Test Accuracy: 47.41%\n",
            "Epoch [2/25], Step [100/782], Loss: 1.5889\n",
            "Epoch [2/25], Step [200/782], Loss: 1.5406\n",
            "Epoch [2/25], Step [300/782], Loss: 1.5270\n",
            "Epoch [2/25], Step [400/782], Loss: 1.4915\n",
            "Epoch [2/25], Step [500/782], Loss: 1.4779\n",
            "Epoch [2/25], Step [600/782], Loss: 1.4369\n",
            "Epoch [2/25], Step [700/782], Loss: 1.4085\n",
            "Epoch [2/25] Test Accuracy: 58.24%\n",
            "Epoch [3/25], Step [100/782], Loss: 1.3737\n",
            "Epoch [3/25], Step [200/782], Loss: 1.3383\n",
            "Epoch [3/25], Step [300/782], Loss: 1.3216\n",
            "Epoch [3/25], Step [400/782], Loss: 1.2790\n",
            "Epoch [3/25], Step [500/782], Loss: 1.2809\n",
            "Epoch [3/25], Step [600/782], Loss: 1.2813\n",
            "Epoch [3/25], Step [700/782], Loss: 1.2861\n",
            "Epoch [3/25] Test Accuracy: 64.21%\n",
            "Epoch [4/25], Step [100/782], Loss: 1.2262\n",
            "Epoch [4/25], Step [200/782], Loss: 1.2076\n",
            "Epoch [4/25], Step [300/782], Loss: 1.2192\n",
            "Epoch [4/25], Step [400/782], Loss: 1.1920\n",
            "Epoch [4/25], Step [500/782], Loss: 1.2047\n",
            "Epoch [4/25], Step [600/782], Loss: 1.1864\n",
            "Epoch [4/25], Step [700/782], Loss: 1.1566\n",
            "Epoch [4/25] Test Accuracy: 67.42%\n",
            "Epoch [5/25], Step [100/782], Loss: 1.1401\n",
            "Epoch [5/25], Step [200/782], Loss: 1.1476\n",
            "Epoch [5/25], Step [300/782], Loss: 1.1348\n",
            "Epoch [5/25], Step [400/782], Loss: 1.1024\n",
            "Epoch [5/25], Step [500/782], Loss: 1.1124\n",
            "Epoch [5/25], Step [600/782], Loss: 1.1180\n",
            "Epoch [5/25], Step [700/782], Loss: 1.0831\n",
            "Epoch [5/25] Test Accuracy: 69.10%\n",
            "Epoch [6/25], Step [100/782], Loss: 1.0814\n",
            "Epoch [6/25], Step [200/782], Loss: 1.0838\n",
            "Epoch [6/25], Step [300/782], Loss: 1.0684\n",
            "Epoch [6/25], Step [400/782], Loss: 1.0449\n",
            "Epoch [6/25], Step [500/782], Loss: 1.0850\n",
            "Epoch [6/25], Step [600/782], Loss: 1.0412\n",
            "Epoch [6/25], Step [700/782], Loss: 1.0297\n",
            "Epoch [6/25] Test Accuracy: 70.28%\n",
            "Epoch [7/25], Step [100/782], Loss: 1.0348\n",
            "Epoch [7/25], Step [200/782], Loss: 1.0326\n",
            "Epoch [7/25], Step [300/782], Loss: 1.0237\n",
            "Epoch [7/25], Step [400/782], Loss: 1.0044\n",
            "Epoch [7/25], Step [500/782], Loss: 1.0091\n",
            "Epoch [7/25], Step [600/782], Loss: 0.9917\n",
            "Epoch [7/25], Step [700/782], Loss: 0.9962\n",
            "Epoch [7/25] Test Accuracy: 71.12%\n",
            "Epoch [8/25], Step [100/782], Loss: 1.0166\n",
            "Epoch [8/25], Step [200/782], Loss: 0.9898\n",
            "Epoch [8/25], Step [300/782], Loss: 0.9491\n",
            "Epoch [8/25], Step [400/782], Loss: 0.9852\n",
            "Epoch [8/25], Step [500/782], Loss: 0.9874\n",
            "Epoch [8/25], Step [600/782], Loss: 0.9649\n",
            "Epoch [8/25], Step [700/782], Loss: 0.9719\n",
            "Epoch [8/25] Test Accuracy: 73.32%\n",
            "Epoch [9/25], Step [100/782], Loss: 0.9551\n",
            "Epoch [9/25], Step [200/782], Loss: 0.9660\n",
            "Epoch [9/25], Step [300/782], Loss: 0.9192\n",
            "Epoch [9/25], Step [400/782], Loss: 0.9327\n",
            "Epoch [9/25], Step [500/782], Loss: 0.9442\n",
            "Epoch [9/25], Step [600/782], Loss: 0.9745\n",
            "Epoch [9/25], Step [700/782], Loss: 0.9443\n",
            "Epoch [9/25] Test Accuracy: 74.63%\n",
            "Epoch [10/25], Step [100/782], Loss: 0.9340\n",
            "Epoch [10/25], Step [200/782], Loss: 0.9270\n",
            "Epoch [10/25], Step [300/782], Loss: 0.9289\n",
            "Epoch [10/25], Step [400/782], Loss: 0.9127\n",
            "Epoch [10/25], Step [500/782], Loss: 0.8977\n",
            "Epoch [10/25], Step [600/782], Loss: 0.8948\n",
            "Epoch [10/25], Step [700/782], Loss: 0.9181\n",
            "Epoch [10/25] Test Accuracy: 74.69%\n",
            "Epoch [11/25], Step [100/782], Loss: 0.8819\n",
            "Epoch [11/25], Step [200/782], Loss: 0.8826\n",
            "Epoch [11/25], Step [300/782], Loss: 0.8560\n",
            "Epoch [11/25], Step [400/782], Loss: 0.8706\n",
            "Epoch [11/25], Step [500/782], Loss: 0.8618\n",
            "Epoch [11/25], Step [600/782], Loss: 0.8680\n",
            "Epoch [11/25], Step [700/782], Loss: 0.8475\n",
            "Epoch [11/25] Test Accuracy: 76.30%\n",
            "Epoch [12/25], Step [100/782], Loss: 0.8448\n",
            "Epoch [12/25], Step [200/782], Loss: 0.8326\n",
            "Epoch [12/25], Step [300/782], Loss: 0.8399\n",
            "Epoch [12/25], Step [400/782], Loss: 0.8445\n",
            "Epoch [12/25], Step [500/782], Loss: 0.8186\n",
            "Epoch [12/25], Step [600/782], Loss: 0.8364\n",
            "Epoch [12/25], Step [700/782], Loss: 0.8312\n",
            "Epoch [12/25] Test Accuracy: 76.33%\n",
            "Epoch [13/25], Step [100/782], Loss: 0.8101\n",
            "Epoch [13/25], Step [200/782], Loss: 0.8111\n",
            "Epoch [13/25], Step [300/782], Loss: 0.8216\n",
            "Epoch [13/25], Step [400/782], Loss: 0.8147\n",
            "Epoch [13/25], Step [500/782], Loss: 0.8114\n",
            "Epoch [13/25], Step [600/782], Loss: 0.8590\n",
            "Epoch [13/25], Step [700/782], Loss: 0.8338\n",
            "Epoch [13/25] Test Accuracy: 76.29%\n",
            "Epoch [14/25], Step [100/782], Loss: 0.7984\n",
            "Epoch [14/25], Step [200/782], Loss: 0.8419\n",
            "Epoch [14/25], Step [300/782], Loss: 0.8072\n",
            "Epoch [14/25], Step [400/782], Loss: 0.8217\n",
            "Epoch [14/25], Step [500/782], Loss: 0.8337\n",
            "Epoch [14/25], Step [600/782], Loss: 0.8016\n",
            "Epoch [14/25], Step [700/782], Loss: 0.8001\n",
            "Epoch [14/25] Test Accuracy: 76.70%\n",
            "Epoch [15/25], Step [100/782], Loss: 0.8166\n",
            "Epoch [15/25], Step [200/782], Loss: 0.8090\n",
            "Epoch [15/25], Step [300/782], Loss: 0.8091\n",
            "Epoch [15/25], Step [400/782], Loss: 0.8309\n",
            "Epoch [15/25], Step [500/782], Loss: 0.8144\n",
            "Epoch [15/25], Step [600/782], Loss: 0.7954\n",
            "Epoch [15/25], Step [700/782], Loss: 0.8093\n",
            "Epoch [15/25] Test Accuracy: 76.87%\n",
            "Epoch [16/25], Step [100/782], Loss: 0.7778\n",
            "Epoch [16/25], Step [200/782], Loss: 0.7902\n",
            "Epoch [16/25], Step [300/782], Loss: 0.8239\n",
            "Epoch [16/25], Step [400/782], Loss: 0.8073\n",
            "Epoch [16/25], Step [500/782], Loss: 0.8289\n",
            "Epoch [16/25], Step [600/782], Loss: 0.8123\n",
            "Epoch [16/25], Step [700/782], Loss: 0.7814\n",
            "Epoch [16/25] Test Accuracy: 77.09%\n",
            "Epoch [17/25], Step [100/782], Loss: 0.8178\n",
            "Epoch [17/25], Step [200/782], Loss: 0.7860\n",
            "Epoch [17/25], Step [300/782], Loss: 0.7985\n",
            "Epoch [17/25], Step [400/782], Loss: 0.7919\n",
            "Epoch [17/25], Step [500/782], Loss: 0.7967\n",
            "Epoch [17/25], Step [600/782], Loss: 0.8157\n",
            "Epoch [17/25], Step [700/782], Loss: 0.8058\n",
            "Epoch [17/25] Test Accuracy: 77.41%\n",
            "Epoch [18/25], Step [100/782], Loss: 0.7883\n",
            "Epoch [18/25], Step [200/782], Loss: 0.8052\n",
            "Epoch [18/25], Step [300/782], Loss: 0.7842\n",
            "Epoch [18/25], Step [400/782], Loss: 0.7737\n",
            "Epoch [18/25], Step [500/782], Loss: 0.8016\n",
            "Epoch [18/25], Step [600/782], Loss: 0.7972\n",
            "Epoch [18/25], Step [700/782], Loss: 0.7986\n",
            "Epoch [18/25] Test Accuracy: 77.41%\n",
            "Epoch [19/25], Step [100/782], Loss: 0.7877\n",
            "Epoch [19/25], Step [200/782], Loss: 0.7739\n",
            "Epoch [19/25], Step [300/782], Loss: 0.7776\n",
            "Epoch [19/25], Step [400/782], Loss: 0.7731\n",
            "Epoch [19/25], Step [500/782], Loss: 0.7824\n",
            "Epoch [19/25], Step [600/782], Loss: 0.7933\n",
            "Epoch [19/25], Step [700/782], Loss: 0.7878\n",
            "Epoch [19/25] Test Accuracy: 77.41%\n",
            "Epoch [20/25], Step [100/782], Loss: 0.7711\n",
            "Epoch [20/25], Step [200/782], Loss: 0.7728\n",
            "Epoch [20/25], Step [300/782], Loss: 0.7708\n",
            "Epoch [20/25], Step [400/782], Loss: 0.7844\n",
            "Epoch [20/25], Step [500/782], Loss: 0.7631\n",
            "Epoch [20/25], Step [600/782], Loss: 0.8073\n",
            "Epoch [20/25], Step [700/782], Loss: 0.7744\n",
            "Epoch [20/25] Test Accuracy: 77.32%\n",
            "Epoch [21/25], Step [100/782], Loss: 0.7615\n",
            "Epoch [21/25], Step [200/782], Loss: 0.7747\n",
            "Epoch [21/25], Step [300/782], Loss: 0.7946\n",
            "Epoch [21/25], Step [400/782], Loss: 0.7784\n",
            "Epoch [21/25], Step [500/782], Loss: 0.7872\n",
            "Epoch [21/25], Step [600/782], Loss: 0.7691\n",
            "Epoch [21/25], Step [700/782], Loss: 0.7757\n",
            "Epoch [21/25] Test Accuracy: 77.54%\n",
            "Epoch [22/25], Step [100/782], Loss: 0.7701\n",
            "Epoch [22/25], Step [200/782], Loss: 0.7780\n",
            "Epoch [22/25], Step [300/782], Loss: 0.7604\n",
            "Epoch [22/25], Step [400/782], Loss: 0.7906\n",
            "Epoch [22/25], Step [500/782], Loss: 0.7561\n",
            "Epoch [22/25], Step [600/782], Loss: 0.7948\n",
            "Epoch [22/25], Step [700/782], Loss: 0.7621\n",
            "Epoch [22/25] Test Accuracy: 77.74%\n",
            "Epoch [23/25], Step [100/782], Loss: 0.7495\n",
            "Epoch [23/25], Step [200/782], Loss: 0.7694\n",
            "Epoch [23/25], Step [300/782], Loss: 0.7619\n",
            "Epoch [23/25], Step [400/782], Loss: 0.7878\n",
            "Epoch [23/25], Step [500/782], Loss: 0.7744\n",
            "Epoch [23/25], Step [600/782], Loss: 0.7440\n",
            "Epoch [23/25], Step [700/782], Loss: 0.8051\n",
            "Epoch [23/25] Test Accuracy: 77.83%\n",
            "Epoch [24/25], Step [100/782], Loss: 0.7652\n",
            "Epoch [24/25], Step [200/782], Loss: 0.7623\n",
            "Epoch [24/25], Step [300/782], Loss: 0.7733\n",
            "Epoch [24/25], Step [400/782], Loss: 0.7954\n",
            "Epoch [24/25], Step [500/782], Loss: 0.7776\n",
            "Epoch [24/25], Step [600/782], Loss: 0.7658\n",
            "Epoch [24/25], Step [700/782], Loss: 0.8036\n",
            "Epoch [24/25] Test Accuracy: 77.89%\n",
            "Epoch [25/25], Step [100/782], Loss: 0.7600\n",
            "Epoch [25/25], Step [200/782], Loss: 0.7659\n",
            "Epoch [25/25], Step [300/782], Loss: 0.7650\n",
            "Epoch [25/25], Step [400/782], Loss: 0.7592\n",
            "Epoch [25/25], Step [500/782], Loss: 0.7843\n",
            "Epoch [25/25], Step [600/782], Loss: 0.7798\n",
            "Epoch [25/25], Step [700/782], Loss: 0.7799\n",
            "Epoch [25/25] Test Accuracy: 77.64%\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Model3, self).__init__()\n",
        "\n",
        "        # Block 1: 32 filters\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "        # Block 2: 64 filters\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        # Block 3: 128 filters\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout(0.25)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 128)  # uses 128 instead of 256\n",
        "        self.dropout_fc = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output layer with 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional Block 1\n",
        "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
        "        x = nn.functional.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Convolutional Block 3\n",
        "        x = nn.functional.relu(self.bn5(self.conv5(x)))\n",
        "        x = nn.functional.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "nWVwPlM6vu9Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Initialize model, loss function, optimizer, and scheduler\n",
        "model = CIFAR10Model3().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce LR by half every 10 epochs\n",
        "\n",
        "EPOCHS = 50\n",
        "best_test_accuracy = 0.0  # Track best model\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:  # Print every 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], \"\n",
        "                  f\"Loss: {running_loss/100:.4f}, Train Acc: {100 * correct_train/total_train:.2f}%\")\n",
        "            running_loss = 0.0  # Reset loss tracking\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute test accuracy, precision, and recall\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    test_precision = precision_score(all_labels, all_preds, average='macro') * 100\n",
        "    test_recall = recall_score(all_labels, all_preds, average='macro') * 100\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Test Acc: {test_accuracy:.2f}%, \"\n",
        "          f\"Precision: {test_precision:.2f}%, Recall: {test_recall:.2f}%\")\n",
        "\n",
        "    # Save best model checkpoint\n",
        "    if test_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = test_accuracy\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"Best model saved with accuracy: {best_test_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1rijZ2wLMN",
        "outputId": "66c65c30-28fd-4d54-ad86-f88df432fed2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/50], Step [100/782], Loss: 1.8671, Train Acc: 30.23%\n",
            "Epoch [1/50], Step [200/782], Loss: 1.5741, Train Acc: 35.89%\n",
            "Epoch [1/50], Step [300/782], Loss: 1.4655, Train Acc: 39.28%\n",
            "Epoch [1/50], Step [400/782], Loss: 1.3722, Train Acc: 42.07%\n",
            "Epoch [1/50], Step [500/782], Loss: 1.2852, Train Acc: 44.35%\n",
            "Epoch [1/50], Step [600/782], Loss: 1.2202, Train Acc: 46.27%\n",
            "Epoch [1/50], Step [700/782], Loss: 1.1730, Train Acc: 47.88%\n",
            "Epoch [1/50] - Test Acc: 58.33%, Precision: 61.33%, Recall: 58.33%\n",
            "Best model saved with accuracy: 58.33%\n",
            "Epoch [2/50], Step [100/782], Loss: 1.0705, Train Acc: 62.47%\n",
            "Epoch [2/50], Step [200/782], Loss: 1.0596, Train Acc: 62.61%\n",
            "Epoch [2/50], Step [300/782], Loss: 1.0346, Train Acc: 63.01%\n",
            "Epoch [2/50], Step [400/782], Loss: 1.0197, Train Acc: 63.22%\n",
            "Epoch [2/50], Step [500/782], Loss: 0.9927, Train Acc: 63.66%\n",
            "Epoch [2/50], Step [600/782], Loss: 0.9638, Train Acc: 64.11%\n",
            "Epoch [2/50], Step [700/782], Loss: 0.9416, Train Acc: 64.52%\n",
            "Epoch [2/50] - Test Acc: 72.21%, Precision: 72.51%, Recall: 72.21%\n",
            "Best model saved with accuracy: 72.21%\n",
            "Epoch [3/50], Step [100/782], Loss: 0.8854, Train Acc: 69.47%\n",
            "Epoch [3/50], Step [200/782], Loss: 0.8619, Train Acc: 69.70%\n",
            "Epoch [3/50], Step [300/782], Loss: 0.8529, Train Acc: 69.85%\n",
            "Epoch [3/50], Step [400/782], Loss: 0.8913, Train Acc: 69.68%\n",
            "Epoch [3/50], Step [500/782], Loss: 0.8260, Train Acc: 69.95%\n",
            "Epoch [3/50], Step [600/782], Loss: 0.8224, Train Acc: 70.23%\n",
            "Epoch [3/50], Step [700/782], Loss: 0.8294, Train Acc: 70.39%\n",
            "Epoch [3/50] - Test Acc: 73.01%, Precision: 74.36%, Recall: 73.01%\n",
            "Best model saved with accuracy: 73.01%\n",
            "Epoch [4/50], Step [100/782], Loss: 0.7720, Train Acc: 72.88%\n",
            "Epoch [4/50], Step [200/782], Loss: 0.8033, Train Acc: 72.38%\n",
            "Epoch [4/50], Step [300/782], Loss: 0.7779, Train Acc: 72.62%\n",
            "Epoch [4/50], Step [400/782], Loss: 0.7907, Train Acc: 72.77%\n",
            "Epoch [4/50], Step [500/782], Loss: 0.7647, Train Acc: 72.79%\n",
            "Epoch [4/50], Step [600/782], Loss: 0.7558, Train Acc: 72.98%\n",
            "Epoch [4/50], Step [700/782], Loss: 0.7626, Train Acc: 73.07%\n",
            "Epoch [4/50] - Test Acc: 76.73%, Precision: 77.24%, Recall: 76.73%\n",
            "Best model saved with accuracy: 76.73%\n",
            "Epoch [5/50], Step [100/782], Loss: 0.7251, Train Acc: 75.20%\n",
            "Epoch [5/50], Step [200/782], Loss: 0.7375, Train Acc: 74.99%\n",
            "Epoch [5/50], Step [300/782], Loss: 0.7018, Train Acc: 75.18%\n",
            "Epoch [5/50], Step [400/782], Loss: 0.7129, Train Acc: 75.25%\n",
            "Epoch [5/50], Step [500/782], Loss: 0.6803, Train Acc: 75.43%\n",
            "Epoch [5/50], Step [600/782], Loss: 0.7275, Train Acc: 75.34%\n",
            "Epoch [5/50], Step [700/782], Loss: 0.7083, Train Acc: 75.44%\n",
            "Epoch [5/50] - Test Acc: 78.09%, Precision: 78.53%, Recall: 78.09%\n",
            "Best model saved with accuracy: 78.09%\n",
            "Epoch [6/50], Step [100/782], Loss: 0.6771, Train Acc: 76.73%\n",
            "Epoch [6/50], Step [200/782], Loss: 0.6748, Train Acc: 76.51%\n",
            "Epoch [6/50], Step [300/782], Loss: 0.6719, Train Acc: 76.60%\n",
            "Epoch [6/50], Step [400/782], Loss: 0.6762, Train Acc: 76.68%\n",
            "Epoch [6/50], Step [500/782], Loss: 0.6641, Train Acc: 76.73%\n",
            "Epoch [6/50], Step [600/782], Loss: 0.6690, Train Acc: 76.72%\n",
            "Epoch [6/50], Step [700/782], Loss: 0.6650, Train Acc: 76.80%\n",
            "Epoch [6/50] - Test Acc: 79.59%, Precision: 79.83%, Recall: 79.59%\n",
            "Best model saved with accuracy: 79.59%\n",
            "Epoch [7/50], Step [100/782], Loss: 0.6439, Train Acc: 77.72%\n",
            "Epoch [7/50], Step [200/782], Loss: 0.6260, Train Acc: 78.27%\n",
            "Epoch [7/50], Step [300/782], Loss: 0.6180, Train Acc: 78.47%\n",
            "Epoch [7/50], Step [400/782], Loss: 0.6228, Train Acc: 78.53%\n",
            "Epoch [7/50], Step [500/782], Loss: 0.6234, Train Acc: 78.48%\n",
            "Epoch [7/50], Step [600/782], Loss: 0.6350, Train Acc: 78.45%\n",
            "Epoch [7/50], Step [700/782], Loss: 0.6475, Train Acc: 78.43%\n",
            "Epoch [7/50] - Test Acc: 81.16%, Precision: 81.63%, Recall: 81.16%\n",
            "Best model saved with accuracy: 81.16%\n",
            "Epoch [8/50], Step [100/782], Loss: 0.5953, Train Acc: 80.39%\n",
            "Epoch [8/50], Step [200/782], Loss: 0.6112, Train Acc: 79.73%\n",
            "Epoch [8/50], Step [300/782], Loss: 0.6188, Train Acc: 79.14%\n",
            "Epoch [8/50], Step [400/782], Loss: 0.6058, Train Acc: 79.28%\n",
            "Epoch [8/50], Step [500/782], Loss: 0.6145, Train Acc: 79.18%\n",
            "Epoch [8/50], Step [600/782], Loss: 0.5886, Train Acc: 79.22%\n",
            "Epoch [8/50], Step [700/782], Loss: 0.6018, Train Acc: 79.25%\n",
            "Epoch [8/50] - Test Acc: 81.05%, Precision: 81.63%, Recall: 81.05%\n",
            "Epoch [9/50], Step [100/782], Loss: 0.5830, Train Acc: 79.92%\n",
            "Epoch [9/50], Step [200/782], Loss: 0.5817, Train Acc: 79.80%\n",
            "Epoch [9/50], Step [300/782], Loss: 0.5485, Train Acc: 80.49%\n",
            "Epoch [9/50], Step [400/782], Loss: 0.6066, Train Acc: 80.32%\n",
            "Epoch [9/50], Step [500/782], Loss: 0.5819, Train Acc: 80.36%\n",
            "Epoch [9/50], Step [600/782], Loss: 0.5454, Train Acc: 80.55%\n",
            "Epoch [9/50], Step [700/782], Loss: 0.5700, Train Acc: 80.54%\n",
            "Epoch [9/50] - Test Acc: 82.05%, Precision: 82.43%, Recall: 82.05%\n",
            "Best model saved with accuracy: 82.05%\n",
            "Epoch [10/50], Step [100/782], Loss: 0.5368, Train Acc: 81.97%\n",
            "Epoch [10/50], Step [200/782], Loss: 0.5414, Train Acc: 81.80%\n",
            "Epoch [10/50], Step [300/782], Loss: 0.5506, Train Acc: 81.76%\n",
            "Epoch [10/50], Step [400/782], Loss: 0.5319, Train Acc: 81.76%\n",
            "Epoch [10/50], Step [500/782], Loss: 0.5626, Train Acc: 81.65%\n",
            "Epoch [10/50], Step [600/782], Loss: 0.5616, Train Acc: 81.53%\n",
            "Epoch [10/50], Step [700/782], Loss: 0.5461, Train Acc: 81.51%\n",
            "Epoch [10/50] - Test Acc: 83.33%, Precision: 83.63%, Recall: 83.33%\n",
            "Best model saved with accuracy: 83.33%\n",
            "Epoch [11/50], Step [100/782], Loss: 0.4939, Train Acc: 83.09%\n",
            "Epoch [11/50], Step [200/782], Loss: 0.4864, Train Acc: 83.20%\n",
            "Epoch [11/50], Step [300/782], Loss: 0.4942, Train Acc: 83.08%\n",
            "Epoch [11/50], Step [400/782], Loss: 0.4799, Train Acc: 83.13%\n",
            "Epoch [11/50], Step [500/782], Loss: 0.4852, Train Acc: 83.20%\n",
            "Epoch [11/50], Step [600/782], Loss: 0.4904, Train Acc: 83.21%\n",
            "Epoch [11/50], Step [700/782], Loss: 0.4731, Train Acc: 83.26%\n",
            "Epoch [11/50] - Test Acc: 84.75%, Precision: 84.74%, Recall: 84.75%\n",
            "Best model saved with accuracy: 84.75%\n",
            "Epoch [12/50], Step [100/782], Loss: 0.4412, Train Acc: 85.05%\n",
            "Epoch [12/50], Step [200/782], Loss: 0.4593, Train Acc: 84.71%\n",
            "Epoch [12/50], Step [300/782], Loss: 0.4690, Train Acc: 84.41%\n",
            "Epoch [12/50], Step [400/782], Loss: 0.4775, Train Acc: 84.25%\n",
            "Epoch [12/50], Step [500/782], Loss: 0.4472, Train Acc: 84.31%\n",
            "Epoch [12/50], Step [600/782], Loss: 0.4681, Train Acc: 84.24%\n",
            "Epoch [12/50], Step [700/782], Loss: 0.4784, Train Acc: 84.18%\n",
            "Epoch [12/50] - Test Acc: 84.62%, Precision: 84.88%, Recall: 84.62%\n",
            "Epoch [13/50], Step [100/782], Loss: 0.4575, Train Acc: 84.12%\n",
            "Epoch [13/50], Step [200/782], Loss: 0.4350, Train Acc: 84.62%\n",
            "Epoch [13/50], Step [300/782], Loss: 0.4469, Train Acc: 84.59%\n",
            "Epoch [13/50], Step [400/782], Loss: 0.4497, Train Acc: 84.55%\n",
            "Epoch [13/50], Step [500/782], Loss: 0.4658, Train Acc: 84.54%\n",
            "Epoch [13/50], Step [600/782], Loss: 0.4429, Train Acc: 84.57%\n",
            "Epoch [13/50], Step [700/782], Loss: 0.4610, Train Acc: 84.56%\n",
            "Epoch [13/50] - Test Acc: 85.51%, Precision: 85.45%, Recall: 85.51%\n",
            "Best model saved with accuracy: 85.51%\n",
            "Epoch [14/50], Step [100/782], Loss: 0.4268, Train Acc: 85.59%\n",
            "Epoch [14/50], Step [200/782], Loss: 0.4491, Train Acc: 85.02%\n",
            "Epoch [14/50], Step [300/782], Loss: 0.4417, Train Acc: 84.85%\n",
            "Epoch [14/50], Step [400/782], Loss: 0.4334, Train Acc: 84.93%\n",
            "Epoch [14/50], Step [500/782], Loss: 0.4318, Train Acc: 84.96%\n",
            "Epoch [14/50], Step [600/782], Loss: 0.4311, Train Acc: 85.02%\n",
            "Epoch [14/50], Step [700/782], Loss: 0.4360, Train Acc: 85.03%\n",
            "Epoch [14/50] - Test Acc: 85.62%, Precision: 85.68%, Recall: 85.62%\n",
            "Best model saved with accuracy: 85.62%\n",
            "Epoch [15/50], Step [100/782], Loss: 0.4160, Train Acc: 85.80%\n",
            "Epoch [15/50], Step [200/782], Loss: 0.4232, Train Acc: 85.80%\n",
            "Epoch [15/50], Step [300/782], Loss: 0.4131, Train Acc: 85.85%\n",
            "Epoch [15/50], Step [400/782], Loss: 0.4359, Train Acc: 85.62%\n",
            "Epoch [15/50], Step [500/782], Loss: 0.4252, Train Acc: 85.66%\n",
            "Epoch [15/50], Step [600/782], Loss: 0.4345, Train Acc: 85.61%\n",
            "Epoch [15/50], Step [700/782], Loss: 0.4243, Train Acc: 85.61%\n",
            "Epoch [15/50] - Test Acc: 85.87%, Precision: 85.90%, Recall: 85.87%\n",
            "Best model saved with accuracy: 85.87%\n",
            "Epoch [16/50], Step [100/782], Loss: 0.4058, Train Acc: 86.33%\n",
            "Epoch [16/50], Step [200/782], Loss: 0.4229, Train Acc: 85.59%\n",
            "Epoch [16/50], Step [300/782], Loss: 0.4106, Train Acc: 85.56%\n",
            "Epoch [16/50], Step [400/782], Loss: 0.4087, Train Acc: 85.68%\n",
            "Epoch [16/50], Step [500/782], Loss: 0.4147, Train Acc: 85.76%\n",
            "Epoch [16/50], Step [600/782], Loss: 0.4147, Train Acc: 85.77%\n",
            "Epoch [16/50], Step [700/782], Loss: 0.3944, Train Acc: 85.92%\n",
            "Epoch [16/50] - Test Acc: 86.00%, Precision: 86.02%, Recall: 86.00%\n",
            "Best model saved with accuracy: 86.00%\n",
            "Epoch [17/50], Step [100/782], Loss: 0.4013, Train Acc: 86.67%\n",
            "Epoch [17/50], Step [200/782], Loss: 0.3960, Train Acc: 86.50%\n",
            "Epoch [17/50], Step [300/782], Loss: 0.3900, Train Acc: 86.52%\n",
            "Epoch [17/50], Step [400/782], Loss: 0.4285, Train Acc: 86.21%\n",
            "Epoch [17/50], Step [500/782], Loss: 0.3966, Train Acc: 86.27%\n",
            "Epoch [17/50], Step [600/782], Loss: 0.4150, Train Acc: 86.21%\n",
            "Epoch [17/50], Step [700/782], Loss: 0.4258, Train Acc: 86.11%\n",
            "Epoch [17/50] - Test Acc: 86.41%, Precision: 86.49%, Recall: 86.41%\n",
            "Best model saved with accuracy: 86.41%\n",
            "Epoch [18/50], Step [100/782], Loss: 0.3979, Train Acc: 86.47%\n",
            "Epoch [18/50], Step [200/782], Loss: 0.3876, Train Acc: 86.75%\n",
            "Epoch [18/50], Step [300/782], Loss: 0.3915, Train Acc: 86.56%\n",
            "Epoch [18/50], Step [400/782], Loss: 0.3982, Train Acc: 86.50%\n",
            "Epoch [18/50], Step [500/782], Loss: 0.3989, Train Acc: 86.41%\n",
            "Epoch [18/50], Step [600/782], Loss: 0.4071, Train Acc: 86.41%\n",
            "Epoch [18/50], Step [700/782], Loss: 0.4079, Train Acc: 86.34%\n",
            "Epoch [18/50] - Test Acc: 85.98%, Precision: 86.19%, Recall: 85.98%\n",
            "Epoch [19/50], Step [100/782], Loss: 0.3780, Train Acc: 87.20%\n",
            "Epoch [19/50], Step [200/782], Loss: 0.3765, Train Acc: 87.02%\n",
            "Epoch [19/50], Step [300/782], Loss: 0.3988, Train Acc: 86.90%\n",
            "Epoch [19/50], Step [400/782], Loss: 0.3918, Train Acc: 86.81%\n",
            "Epoch [19/50], Step [500/782], Loss: 0.3953, Train Acc: 86.78%\n",
            "Epoch [19/50], Step [600/782], Loss: 0.3832, Train Acc: 86.83%\n",
            "Epoch [19/50], Step [700/782], Loss: 0.3976, Train Acc: 86.74%\n",
            "Epoch [19/50] - Test Acc: 86.57%, Precision: 86.51%, Recall: 86.57%\n",
            "Best model saved with accuracy: 86.57%\n",
            "Epoch [20/50], Step [100/782], Loss: 0.3692, Train Acc: 87.09%\n",
            "Epoch [20/50], Step [200/782], Loss: 0.3703, Train Acc: 87.27%\n",
            "Epoch [20/50], Step [300/782], Loss: 0.3978, Train Acc: 86.96%\n",
            "Epoch [20/50], Step [400/782], Loss: 0.3832, Train Acc: 86.89%\n",
            "Epoch [20/50], Step [500/782], Loss: 0.3677, Train Acc: 86.97%\n",
            "Epoch [20/50], Step [600/782], Loss: 0.3776, Train Acc: 87.05%\n",
            "Epoch [20/50], Step [700/782], Loss: 0.3897, Train Acc: 87.00%\n",
            "Epoch [20/50] - Test Acc: 86.85%, Precision: 86.91%, Recall: 86.85%\n",
            "Best model saved with accuracy: 86.85%\n",
            "Epoch [21/50], Step [100/782], Loss: 0.3627, Train Acc: 87.55%\n",
            "Epoch [21/50], Step [200/782], Loss: 0.3430, Train Acc: 87.99%\n",
            "Epoch [21/50], Step [300/782], Loss: 0.3487, Train Acc: 88.02%\n",
            "Epoch [21/50], Step [400/782], Loss: 0.3566, Train Acc: 87.89%\n",
            "Epoch [21/50], Step [500/782], Loss: 0.3544, Train Acc: 87.93%\n",
            "Epoch [21/50], Step [600/782], Loss: 0.3472, Train Acc: 87.91%\n",
            "Epoch [21/50], Step [700/782], Loss: 0.3513, Train Acc: 87.94%\n",
            "Epoch [21/50] - Test Acc: 87.47%, Precision: 87.48%, Recall: 87.47%\n",
            "Best model saved with accuracy: 87.47%\n",
            "Epoch [22/50], Step [100/782], Loss: 0.3495, Train Acc: 87.92%\n",
            "Epoch [22/50], Step [200/782], Loss: 0.3543, Train Acc: 87.77%\n",
            "Epoch [22/50], Step [300/782], Loss: 0.3323, Train Acc: 87.98%\n",
            "Epoch [22/50], Step [400/782], Loss: 0.3468, Train Acc: 87.98%\n",
            "Epoch [22/50], Step [500/782], Loss: 0.3425, Train Acc: 88.12%\n",
            "Epoch [22/50], Step [600/782], Loss: 0.3371, Train Acc: 88.21%\n",
            "Epoch [22/50], Step [700/782], Loss: 0.3366, Train Acc: 88.29%\n",
            "Epoch [22/50] - Test Acc: 87.29%, Precision: 87.39%, Recall: 87.29%\n",
            "Epoch [23/50], Step [100/782], Loss: 0.3374, Train Acc: 88.80%\n",
            "Epoch [23/50], Step [200/782], Loss: 0.3314, Train Acc: 88.66%\n",
            "Epoch [23/50], Step [300/782], Loss: 0.3216, Train Acc: 89.01%\n",
            "Epoch [23/50], Step [400/782], Loss: 0.3284, Train Acc: 88.93%\n",
            "Epoch [23/50], Step [500/782], Loss: 0.3400, Train Acc: 88.76%\n",
            "Epoch [23/50], Step [600/782], Loss: 0.3246, Train Acc: 88.76%\n",
            "Epoch [23/50], Step [700/782], Loss: 0.3643, Train Acc: 88.59%\n",
            "Epoch [23/50] - Test Acc: 87.42%, Precision: 87.38%, Recall: 87.42%\n",
            "Epoch [24/50], Step [100/782], Loss: 0.3301, Train Acc: 88.50%\n",
            "Epoch [24/50], Step [200/782], Loss: 0.3207, Train Acc: 88.88%\n",
            "Epoch [24/50], Step [300/782], Loss: 0.3210, Train Acc: 88.90%\n",
            "Epoch [24/50], Step [400/782], Loss: 0.3408, Train Acc: 88.76%\n",
            "Epoch [24/50], Step [500/782], Loss: 0.3207, Train Acc: 88.78%\n",
            "Epoch [24/50], Step [600/782], Loss: 0.3394, Train Acc: 88.74%\n",
            "Epoch [24/50], Step [700/782], Loss: 0.3328, Train Acc: 88.63%\n",
            "Epoch [24/50] - Test Acc: 87.57%, Precision: 87.54%, Recall: 87.57%\n",
            "Best model saved with accuracy: 87.57%\n",
            "Epoch [25/50], Step [100/782], Loss: 0.3082, Train Acc: 89.53%\n",
            "Epoch [25/50], Step [200/782], Loss: 0.3146, Train Acc: 89.40%\n",
            "Epoch [25/50], Step [300/782], Loss: 0.3306, Train Acc: 89.09%\n",
            "Epoch [25/50], Step [400/782], Loss: 0.3189, Train Acc: 89.05%\n",
            "Epoch [25/50], Step [500/782], Loss: 0.3459, Train Acc: 88.96%\n",
            "Epoch [25/50], Step [600/782], Loss: 0.3168, Train Acc: 88.98%\n",
            "Epoch [25/50], Step [700/782], Loss: 0.3350, Train Acc: 88.93%\n",
            "Epoch [25/50] - Test Acc: 87.39%, Precision: 87.32%, Recall: 87.39%\n",
            "Epoch [26/50], Step [100/782], Loss: 0.3154, Train Acc: 88.98%\n",
            "Epoch [26/50], Step [200/782], Loss: 0.3207, Train Acc: 89.16%\n",
            "Epoch [26/50], Step [300/782], Loss: 0.3183, Train Acc: 89.16%\n",
            "Epoch [26/50], Step [400/782], Loss: 0.3243, Train Acc: 89.06%\n",
            "Epoch [26/50], Step [500/782], Loss: 0.3039, Train Acc: 89.18%\n",
            "Epoch [26/50], Step [600/782], Loss: 0.3327, Train Acc: 88.98%\n",
            "Epoch [26/50], Step [700/782], Loss: 0.3164, Train Acc: 88.92%\n",
            "Epoch [26/50] - Test Acc: 87.46%, Precision: 87.49%, Recall: 87.46%\n",
            "Epoch [27/50], Step [100/782], Loss: 0.3180, Train Acc: 89.27%\n",
            "Epoch [27/50], Step [200/782], Loss: 0.3148, Train Acc: 89.05%\n",
            "Epoch [27/50], Step [300/782], Loss: 0.3213, Train Acc: 89.09%\n",
            "Epoch [27/50], Step [400/782], Loss: 0.3206, Train Acc: 89.14%\n",
            "Epoch [27/50], Step [500/782], Loss: 0.3264, Train Acc: 89.03%\n",
            "Epoch [27/50], Step [600/782], Loss: 0.3249, Train Acc: 88.94%\n",
            "Epoch [27/50], Step [700/782], Loss: 0.3173, Train Acc: 88.96%\n",
            "Epoch [27/50] - Test Acc: 87.66%, Precision: 87.64%, Recall: 87.66%\n",
            "Best model saved with accuracy: 87.66%\n",
            "Epoch [28/50], Step [100/782], Loss: 0.3084, Train Acc: 89.47%\n",
            "Epoch [28/50], Step [200/782], Loss: 0.3063, Train Acc: 89.42%\n",
            "Epoch [28/50], Step [300/782], Loss: 0.3206, Train Acc: 89.30%\n",
            "Epoch [28/50], Step [400/782], Loss: 0.3156, Train Acc: 89.27%\n",
            "Epoch [28/50], Step [500/782], Loss: 0.3034, Train Acc: 89.27%\n",
            "Epoch [28/50], Step [600/782], Loss: 0.3100, Train Acc: 89.26%\n",
            "Epoch [28/50], Step [700/782], Loss: 0.3064, Train Acc: 89.24%\n",
            "Epoch [28/50] - Test Acc: 87.43%, Precision: 87.50%, Recall: 87.43%\n",
            "Epoch [29/50], Step [100/782], Loss: 0.3072, Train Acc: 89.17%\n",
            "Epoch [29/50], Step [200/782], Loss: 0.3317, Train Acc: 88.84%\n",
            "Epoch [29/50], Step [300/782], Loss: 0.2953, Train Acc: 89.13%\n",
            "Epoch [29/50], Step [400/782], Loss: 0.2976, Train Acc: 89.27%\n",
            "Epoch [29/50], Step [500/782], Loss: 0.3019, Train Acc: 89.32%\n",
            "Epoch [29/50], Step [600/782], Loss: 0.3009, Train Acc: 89.33%\n",
            "Epoch [29/50], Step [700/782], Loss: 0.3123, Train Acc: 89.31%\n",
            "Epoch [29/50] - Test Acc: 87.78%, Precision: 87.84%, Recall: 87.78%\n",
            "Best model saved with accuracy: 87.78%\n",
            "Epoch [30/50], Step [100/782], Loss: 0.3061, Train Acc: 89.77%\n",
            "Epoch [30/50], Step [200/782], Loss: 0.3048, Train Acc: 89.68%\n",
            "Epoch [30/50], Step [300/782], Loss: 0.3020, Train Acc: 89.76%\n",
            "Epoch [30/50], Step [400/782], Loss: 0.2965, Train Acc: 89.75%\n",
            "Epoch [30/50], Step [500/782], Loss: 0.3146, Train Acc: 89.66%\n",
            "Epoch [30/50], Step [600/782], Loss: 0.3052, Train Acc: 89.62%\n",
            "Epoch [30/50], Step [700/782], Loss: 0.3094, Train Acc: 89.59%\n",
            "Epoch [30/50] - Test Acc: 87.31%, Precision: 87.34%, Recall: 87.31%\n",
            "Epoch [31/50], Step [100/782], Loss: 0.2913, Train Acc: 89.91%\n",
            "Epoch [31/50], Step [200/782], Loss: 0.2877, Train Acc: 90.01%\n",
            "Epoch [31/50], Step [300/782], Loss: 0.2866, Train Acc: 89.92%\n",
            "Epoch [31/50], Step [400/782], Loss: 0.2745, Train Acc: 90.16%\n",
            "Epoch [31/50], Step [500/782], Loss: 0.3021, Train Acc: 90.03%\n",
            "Epoch [31/50], Step [600/782], Loss: 0.2864, Train Acc: 90.03%\n",
            "Epoch [31/50], Step [700/782], Loss: 0.2971, Train Acc: 89.97%\n",
            "Epoch [31/50] - Test Acc: 87.86%, Precision: 87.83%, Recall: 87.86%\n",
            "Best model saved with accuracy: 87.86%\n",
            "Epoch [32/50], Step [100/782], Loss: 0.2849, Train Acc: 90.19%\n",
            "Epoch [32/50], Step [200/782], Loss: 0.2734, Train Acc: 90.43%\n",
            "Epoch [32/50], Step [300/782], Loss: 0.2915, Train Acc: 90.16%\n",
            "Epoch [32/50], Step [400/782], Loss: 0.2778, Train Acc: 90.20%\n",
            "Epoch [32/50], Step [500/782], Loss: 0.2926, Train Acc: 90.17%\n",
            "Epoch [32/50], Step [600/782], Loss: 0.2744, Train Acc: 90.23%\n",
            "Epoch [32/50], Step [700/782], Loss: 0.2890, Train Acc: 90.19%\n",
            "Epoch [32/50] - Test Acc: 88.12%, Precision: 88.10%, Recall: 88.12%\n",
            "Best model saved with accuracy: 88.12%\n",
            "Epoch [33/50], Step [100/782], Loss: 0.2896, Train Acc: 89.48%\n",
            "Epoch [33/50], Step [200/782], Loss: 0.2708, Train Acc: 90.15%\n",
            "Epoch [33/50], Step [300/782], Loss: 0.2809, Train Acc: 90.28%\n",
            "Epoch [33/50], Step [400/782], Loss: 0.2834, Train Acc: 90.16%\n",
            "Epoch [33/50], Step [500/782], Loss: 0.2778, Train Acc: 90.13%\n",
            "Epoch [33/50], Step [600/782], Loss: 0.2675, Train Acc: 90.22%\n",
            "Epoch [33/50], Step [700/782], Loss: 0.2786, Train Acc: 90.25%\n",
            "Epoch [33/50] - Test Acc: 87.85%, Precision: 87.90%, Recall: 87.85%\n",
            "Epoch [34/50], Step [100/782], Loss: 0.2647, Train Acc: 91.11%\n",
            "Epoch [34/50], Step [200/782], Loss: 0.2822, Train Acc: 90.56%\n",
            "Epoch [34/50], Step [300/782], Loss: 0.2851, Train Acc: 90.39%\n",
            "Epoch [34/50], Step [400/782], Loss: 0.2835, Train Acc: 90.37%\n",
            "Epoch [34/50], Step [500/782], Loss: 0.2836, Train Acc: 90.39%\n",
            "Epoch [34/50], Step [600/782], Loss: 0.2969, Train Acc: 90.27%\n",
            "Epoch [34/50], Step [700/782], Loss: 0.2687, Train Acc: 90.33%\n",
            "Epoch [34/50] - Test Acc: 87.86%, Precision: 87.81%, Recall: 87.86%\n",
            "Epoch [35/50], Step [100/782], Loss: 0.2666, Train Acc: 90.97%\n",
            "Epoch [35/50], Step [200/782], Loss: 0.2686, Train Acc: 90.67%\n",
            "Epoch [35/50], Step [300/782], Loss: 0.2711, Train Acc: 90.58%\n",
            "Epoch [35/50], Step [400/782], Loss: 0.2741, Train Acc: 90.52%\n",
            "Epoch [35/50], Step [500/782], Loss: 0.2777, Train Acc: 90.43%\n",
            "Epoch [35/50], Step [600/782], Loss: 0.2691, Train Acc: 90.47%\n",
            "Epoch [35/50], Step [700/782], Loss: 0.2735, Train Acc: 90.51%\n",
            "Epoch [35/50] - Test Acc: 88.14%, Precision: 88.12%, Recall: 88.14%\n",
            "Best model saved with accuracy: 88.14%\n",
            "Epoch [36/50], Step [100/782], Loss: 0.2656, Train Acc: 90.88%\n",
            "Epoch [36/50], Step [200/782], Loss: 0.2697, Train Acc: 90.74%\n",
            "Epoch [36/50], Step [300/782], Loss: 0.2662, Train Acc: 90.65%\n",
            "Epoch [36/50], Step [400/782], Loss: 0.2863, Train Acc: 90.50%\n",
            "Epoch [36/50], Step [500/782], Loss: 0.2823, Train Acc: 90.40%\n",
            "Epoch [36/50], Step [600/782], Loss: 0.2576, Train Acc: 90.54%\n",
            "Epoch [36/50], Step [700/782], Loss: 0.2794, Train Acc: 90.51%\n",
            "Epoch [36/50] - Test Acc: 88.06%, Precision: 88.00%, Recall: 88.06%\n",
            "Epoch [37/50], Step [100/782], Loss: 0.2826, Train Acc: 90.02%\n",
            "Epoch [37/50], Step [200/782], Loss: 0.2646, Train Acc: 90.55%\n",
            "Epoch [37/50], Step [300/782], Loss: 0.2615, Train Acc: 90.61%\n",
            "Epoch [37/50], Step [400/782], Loss: 0.2551, Train Acc: 90.73%\n",
            "Epoch [37/50], Step [500/782], Loss: 0.2631, Train Acc: 90.79%\n",
            "Epoch [37/50], Step [600/782], Loss: 0.2780, Train Acc: 90.71%\n",
            "Epoch [37/50], Step [700/782], Loss: 0.2729, Train Acc: 90.73%\n",
            "Epoch [37/50] - Test Acc: 88.16%, Precision: 88.13%, Recall: 88.16%\n",
            "Best model saved with accuracy: 88.16%\n",
            "Epoch [38/50], Step [100/782], Loss: 0.2635, Train Acc: 90.70%\n",
            "Epoch [38/50], Step [200/782], Loss: 0.2723, Train Acc: 90.56%\n",
            "Epoch [38/50], Step [300/782], Loss: 0.2601, Train Acc: 90.68%\n",
            "Epoch [38/50], Step [400/782], Loss: 0.2631, Train Acc: 90.69%\n",
            "Epoch [38/50], Step [500/782], Loss: 0.2716, Train Acc: 90.76%\n",
            "Epoch [38/50], Step [600/782], Loss: 0.2683, Train Acc: 90.65%\n",
            "Epoch [38/50], Step [700/782], Loss: 0.2822, Train Acc: 90.60%\n",
            "Epoch [38/50] - Test Acc: 88.27%, Precision: 88.22%, Recall: 88.27%\n",
            "Best model saved with accuracy: 88.27%\n",
            "Epoch [39/50], Step [100/782], Loss: 0.2631, Train Acc: 91.36%\n",
            "Epoch [39/50], Step [200/782], Loss: 0.2872, Train Acc: 90.56%\n",
            "Epoch [39/50], Step [300/782], Loss: 0.2665, Train Acc: 90.61%\n",
            "Epoch [39/50], Step [400/782], Loss: 0.2579, Train Acc: 90.60%\n",
            "Epoch [39/50], Step [500/782], Loss: 0.2729, Train Acc: 90.62%\n",
            "Epoch [39/50], Step [600/782], Loss: 0.2581, Train Acc: 90.71%\n",
            "Epoch [39/50], Step [700/782], Loss: 0.2781, Train Acc: 90.69%\n",
            "Epoch [39/50] - Test Acc: 88.41%, Precision: 88.34%, Recall: 88.41%\n",
            "Best model saved with accuracy: 88.41%\n",
            "Epoch [40/50], Step [100/782], Loss: 0.2734, Train Acc: 90.27%\n",
            "Epoch [40/50], Step [200/782], Loss: 0.2758, Train Acc: 90.37%\n",
            "Epoch [40/50], Step [300/782], Loss: 0.2715, Train Acc: 90.40%\n",
            "Epoch [40/50], Step [400/782], Loss: 0.2660, Train Acc: 90.43%\n",
            "Epoch [40/50], Step [500/782], Loss: 0.2691, Train Acc: 90.49%\n",
            "Epoch [40/50], Step [600/782], Loss: 0.2680, Train Acc: 90.55%\n",
            "Epoch [40/50], Step [700/782], Loss: 0.2540, Train Acc: 90.66%\n",
            "Epoch [40/50] - Test Acc: 88.32%, Precision: 88.32%, Recall: 88.32%\n",
            "Epoch [41/50], Step [100/782], Loss: 0.2571, Train Acc: 91.03%\n",
            "Epoch [41/50], Step [200/782], Loss: 0.2640, Train Acc: 90.88%\n",
            "Epoch [41/50], Step [300/782], Loss: 0.2502, Train Acc: 91.07%\n",
            "Epoch [41/50], Step [400/782], Loss: 0.2518, Train Acc: 91.06%\n",
            "Epoch [41/50], Step [500/782], Loss: 0.2543, Train Acc: 91.08%\n",
            "Epoch [41/50], Step [600/782], Loss: 0.2651, Train Acc: 91.03%\n",
            "Epoch [41/50], Step [700/782], Loss: 0.2657, Train Acc: 90.95%\n",
            "Epoch [41/50] - Test Acc: 88.39%, Precision: 88.36%, Recall: 88.39%\n",
            "Epoch [42/50], Step [100/782], Loss: 0.2548, Train Acc: 91.09%\n",
            "Epoch [42/50], Step [200/782], Loss: 0.2402, Train Acc: 91.48%\n",
            "Epoch [42/50], Step [300/782], Loss: 0.2486, Train Acc: 91.28%\n",
            "Epoch [42/50], Step [400/782], Loss: 0.2726, Train Acc: 91.04%\n",
            "Epoch [42/50], Step [500/782], Loss: 0.2638, Train Acc: 91.01%\n",
            "Epoch [42/50], Step [600/782], Loss: 0.2481, Train Acc: 91.05%\n",
            "Epoch [42/50], Step [700/782], Loss: 0.2631, Train Acc: 91.01%\n",
            "Epoch [42/50] - Test Acc: 88.37%, Precision: 88.34%, Recall: 88.37%\n",
            "Epoch [43/50], Step [100/782], Loss: 0.2499, Train Acc: 91.28%\n",
            "Epoch [43/50], Step [200/782], Loss: 0.2586, Train Acc: 91.20%\n",
            "Epoch [43/50], Step [300/782], Loss: 0.2617, Train Acc: 91.12%\n",
            "Epoch [43/50], Step [400/782], Loss: 0.2631, Train Acc: 90.97%\n",
            "Epoch [43/50], Step [500/782], Loss: 0.2699, Train Acc: 90.87%\n",
            "Epoch [43/50], Step [600/782], Loss: 0.2601, Train Acc: 90.85%\n",
            "Epoch [43/50], Step [700/782], Loss: 0.2440, Train Acc: 90.91%\n",
            "Epoch [43/50] - Test Acc: 88.42%, Precision: 88.40%, Recall: 88.42%\n",
            "Best model saved with accuracy: 88.42%\n",
            "Epoch [44/50], Step [100/782], Loss: 0.2676, Train Acc: 90.55%\n",
            "Epoch [44/50], Step [200/782], Loss: 0.2496, Train Acc: 90.91%\n",
            "Epoch [44/50], Step [300/782], Loss: 0.2535, Train Acc: 91.00%\n",
            "Epoch [44/50], Step [400/782], Loss: 0.2487, Train Acc: 91.02%\n",
            "Epoch [44/50], Step [500/782], Loss: 0.2569, Train Acc: 91.03%\n",
            "Epoch [44/50], Step [600/782], Loss: 0.2659, Train Acc: 91.01%\n",
            "Epoch [44/50], Step [700/782], Loss: 0.2615, Train Acc: 91.05%\n",
            "Epoch [44/50] - Test Acc: 88.62%, Precision: 88.61%, Recall: 88.62%\n",
            "Best model saved with accuracy: 88.62%\n",
            "Epoch [45/50], Step [100/782], Loss: 0.2633, Train Acc: 90.72%\n",
            "Epoch [45/50], Step [200/782], Loss: 0.2480, Train Acc: 90.97%\n",
            "Epoch [45/50], Step [300/782], Loss: 0.2499, Train Acc: 91.12%\n",
            "Epoch [45/50], Step [400/782], Loss: 0.2600, Train Acc: 91.15%\n",
            "Epoch [45/50], Step [500/782], Loss: 0.2472, Train Acc: 91.17%\n",
            "Epoch [45/50], Step [600/782], Loss: 0.2498, Train Acc: 91.21%\n",
            "Epoch [45/50], Step [700/782], Loss: 0.2593, Train Acc: 91.20%\n",
            "Epoch [45/50] - Test Acc: 88.44%, Precision: 88.41%, Recall: 88.44%\n",
            "Epoch [46/50], Step [100/782], Loss: 0.2413, Train Acc: 91.80%\n",
            "Epoch [46/50], Step [200/782], Loss: 0.2533, Train Acc: 91.49%\n",
            "Epoch [46/50], Step [300/782], Loss: 0.2475, Train Acc: 91.61%\n",
            "Epoch [46/50], Step [400/782], Loss: 0.2496, Train Acc: 91.60%\n",
            "Epoch [46/50], Step [500/782], Loss: 0.2508, Train Acc: 91.53%\n",
            "Epoch [46/50], Step [600/782], Loss: 0.2532, Train Acc: 91.44%\n",
            "Epoch [46/50], Step [700/782], Loss: 0.2569, Train Acc: 91.42%\n",
            "Epoch [46/50] - Test Acc: 88.17%, Precision: 88.17%, Recall: 88.17%\n",
            "Epoch [47/50], Step [100/782], Loss: 0.2630, Train Acc: 90.86%\n",
            "Epoch [47/50], Step [200/782], Loss: 0.2492, Train Acc: 91.23%\n",
            "Epoch [47/50], Step [300/782], Loss: 0.2500, Train Acc: 91.12%\n",
            "Epoch [47/50], Step [400/782], Loss: 0.2549, Train Acc: 91.16%\n",
            "Epoch [47/50], Step [500/782], Loss: 0.2455, Train Acc: 91.16%\n",
            "Epoch [47/50], Step [600/782], Loss: 0.2396, Train Acc: 91.27%\n",
            "Epoch [47/50], Step [700/782], Loss: 0.2686, Train Acc: 91.17%\n",
            "Epoch [47/50] - Test Acc: 88.37%, Precision: 88.36%, Recall: 88.37%\n",
            "Epoch [48/50], Step [100/782], Loss: 0.2530, Train Acc: 91.08%\n",
            "Epoch [48/50], Step [200/782], Loss: 0.2378, Train Acc: 91.40%\n",
            "Epoch [48/50], Step [300/782], Loss: 0.2542, Train Acc: 91.30%\n",
            "Epoch [48/50], Step [400/782], Loss: 0.2585, Train Acc: 91.25%\n",
            "Epoch [48/50], Step [500/782], Loss: 0.2541, Train Acc: 91.22%\n",
            "Epoch [48/50], Step [600/782], Loss: 0.2497, Train Acc: 91.21%\n",
            "Epoch [48/50], Step [700/782], Loss: 0.2268, Train Acc: 91.37%\n",
            "Epoch [48/50] - Test Acc: 88.41%, Precision: 88.37%, Recall: 88.41%\n",
            "Epoch [49/50], Step [100/782], Loss: 0.2348, Train Acc: 91.94%\n",
            "Epoch [49/50], Step [200/782], Loss: 0.2472, Train Acc: 91.51%\n",
            "Epoch [49/50], Step [300/782], Loss: 0.2555, Train Acc: 91.41%\n",
            "Epoch [49/50], Step [400/782], Loss: 0.2623, Train Acc: 91.28%\n",
            "Epoch [49/50], Step [500/782], Loss: 0.2545, Train Acc: 91.18%\n",
            "Epoch [49/50], Step [600/782], Loss: 0.2497, Train Acc: 91.17%\n",
            "Epoch [49/50], Step [700/782], Loss: 0.2466, Train Acc: 91.17%\n",
            "Epoch [49/50] - Test Acc: 88.34%, Precision: 88.35%, Recall: 88.34%\n",
            "Epoch [50/50], Step [100/782], Loss: 0.2398, Train Acc: 91.52%\n",
            "Epoch [50/50], Step [200/782], Loss: 0.2376, Train Acc: 91.70%\n",
            "Epoch [50/50], Step [300/782], Loss: 0.2488, Train Acc: 91.55%\n",
            "Epoch [50/50], Step [400/782], Loss: 0.2557, Train Acc: 91.48%\n",
            "Epoch [50/50], Step [500/782], Loss: 0.2615, Train Acc: 91.42%\n",
            "Epoch [50/50], Step [600/782], Loss: 0.2515, Train Acc: 91.43%\n",
            "Epoch [50/50], Step [700/782], Loss: 0.2458, Train Acc: 91.46%\n",
            "Epoch [50/50] - Test Acc: 88.45%, Precision: 88.45%, Recall: 88.45%\n",
            "Training Complete!\n"
          ]
        }
      ]
    }
  ]
}